"""
Enhanced Main Execution Script with Resume Capability - FIXED VERSION
Detects existing trained models and resumes from the appropriate step
"""

import os
import sys
import traceback
import warnings
import argparse
import glob
from datetime import datetime
import numpy as np
import pandas as pd
import tensorflow as tf
from typing import Dict, Tuple, List, Any, Optional, Union

# Add src directory to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'config'))

# Import configuration and modules
from config import Config
from src.utils import setup_logging, print_system_info, ProgressTracker, validate_file_paths
from src.data_loader import CurrencyDataLoader
from src.preprocessing import ForexDataPreprocessor
from src.multi_currency_integration import MultiCurrencyIntegrator
from src.sequence_preparation import SequenceDataPreparator
from src.model_architecture import CNNLSTMArchitecture
from src.training import ModelTrainer
from src.trading_strategy import TradingStrategyManager
from src.visualization import ResultsAnalyzer

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')
tf.get_logger().setLevel('ERROR')

class ExperimentManager:
    """
    à¸ˆà¸±à¸”à¸à¸²à¸£à¸à¸²à¸£ resume à¹à¸¥à¸°à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸š experiment à¸—à¸µà¹ˆà¸¡à¸µà¸­à¸¢à¸¹à¹ˆ
    """
    
    def __init__(self, config):
        self.config = config
        self.logger = setup_logging(config.LOG_LEVEL, config.get_log_path('experiment_manager'))
        
    def find_existing_experiments(self) -> List[Dict]:
        """
        à¸„à¹‰à¸™à¸«à¸² experiment à¸—à¸µà¹ˆà¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¸°à¸ªà¸–à¸²à¸™à¸°à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸° experiment
        
        Returns:
            à¸£à¸²à¸¢à¸à¸²à¸£ experiment à¸à¸£à¹‰à¸­à¸¡à¸ªà¸–à¸²à¸™à¸°
        """
        experiments = []
        
        # à¸„à¹‰à¸™à¸«à¸²à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸µà¹ˆà¹€à¸—à¸£à¸™à¹à¸¥à¹‰à¸§
        model_pattern = os.path.join(self.config.MODELS_DIR, "*_best_model.h5")
        model_files = glob.glob(model_pattern)
        
        for model_path in model_files:
            model_filename = os.path.basename(model_path)
            experiment_id = model_filename.replace('_best_model.h5', '')
            
            # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹„à¸Ÿà¸¥à¹Œ log à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡
            log_dir = os.path.join(self.config.LOGS_DIR, f"experiment_{experiment_id}")
            
            experiment_info = {
                'experiment_id': experiment_id,
                'model_path': model_path,
                'log_dir': log_dir,
                'has_training_completed': os.path.exists(model_path),
                'has_logs': os.path.exists(log_dir),
                'model_size_mb': os.path.getsize(model_path) / (1024*1024) if os.path.exists(model_path) else 0,
                'created_time': datetime.fromtimestamp(os.path.getctime(model_path)) if os.path.exists(model_path) else None
            }
            
            # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸² step à¹„à¸«à¸™à¸—à¸³à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§à¸šà¹‰à¸²à¸‡
            experiment_info['completed_steps'] = self._analyze_completed_steps(experiment_id, log_dir)
            experiments.append(experiment_info)
        
        # à¹€à¸£à¸µà¸¢à¸‡à¸•à¸²à¸¡à¹€à¸§à¸¥à¸²à¸¥à¹ˆà¸²à¸ªà¸¸à¸”
        experiments.sort(key=lambda x: x['created_time'] or datetime.min, reverse=True)
        
        return experiments
    
    def _analyze_completed_steps(self, experiment_id: str, log_dir: str) -> Dict:
        """
        à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸§à¹ˆà¸² step à¹„à¸«à¸™à¸—à¸³à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§à¸šà¹‰à¸²à¸‡
        
        Args:
            experiment_id: ID à¸‚à¸­à¸‡ experiment
            log_dir: à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ log
            
        Returns:
            Dictionary à¸£à¸°à¸šà¸¸ step à¸—à¸µà¹ˆà¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§
        """
        completed = {
            'data_loading': False,
            'preprocessing': False,
            'integration': False,
            'sequence_preparation': False,
            'model_architecture': False,
            'model_training': False,
            'strategy_testing': False,
            'visualization': False
        }
        
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸à¸²à¸£à¹€à¸—à¸£à¸™à¹‚à¸¡à¹€à¸”à¸¥
        model_path = os.path.join(self.config.MODELS_DIR, f"{experiment_id}_best_model.h5")
        if os.path.exists(model_path):
            completed['model_training'] = True
            # à¸–à¹‰à¸²à¹‚à¸¡à¹€à¸”à¸¥à¹€à¸—à¸£à¸™à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§ à¹à¸ªà¸”à¸‡à¸§à¹ˆà¸² step à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²à¹€à¸ªà¸£à¹‡à¸ˆà¸«à¸¡à¸”à¹à¸¥à¹‰à¸§
            completed.update({
                'data_loading': True,
                'preprocessing': True,
                'integration': True,
                'sequence_preparation': True,
                'model_architecture': True
            })
        
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹„à¸Ÿà¸¥à¹Œà¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸­à¸·à¹ˆà¸™à¹†
        results_files = {
            'strategy_testing': ['strategy_performance_comparison_validation.png', 'confusion_matrices_validation.png'],
            'visualization': ['forex_analysis_dashboard_validation.html', 'research_report_*.html']
        }
        
        for step, files in results_files.items():
            for file_pattern in files:
                matching_files = glob.glob(os.path.join(self.config.RESULTS_DIR, file_pattern))
                if matching_files:
                    completed[step] = True
                    break
        
        return completed
    
    def get_latest_experiment(self) -> Optional[Dict]:
        """
        à¸«à¸² experiment à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸—à¸µà¹ˆà¸¡à¸µà¹‚à¸¡à¹€à¸”à¸¥à¹€à¸—à¸£à¸™à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§
        
        Returns:
            à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ experiment à¸¥à¹ˆà¸²à¸ªà¸¸à¸” à¸«à¸£à¸·à¸­ None à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µ
        """
        experiments = self.find_existing_experiments()
        
        for exp in experiments:
            if exp['has_training_completed']:
                return exp
        
        return None
    
    def load_experiment_data(self, experiment_id: str) -> Dict:
        """
        à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£ resume experiment
        
        Args:
            experiment_id: ID à¸‚à¸­à¸‡ experiment à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹‚à¸«à¸¥à¸”
            
        Returns:
            Dictionary à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹‚à¸«à¸¥à¸”à¹„à¸”à¹‰
        """
        self.logger.info(f"Loading experiment data for: {experiment_id}")
        
        experiment_data = {
            'experiment_id': experiment_id,
            'model_path': None,
            'training_data': None,
            'model_config': None,
            'preprocessing_artifacts': None
        }
        
        # à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸µà¹ˆà¹€à¸—à¸£à¸™à¹à¸¥à¹‰à¸§
        model_path = os.path.join(self.config.MODELS_DIR, f"{experiment_id}_best_model.h5")
        if os.path.exists(model_path):
            try:
                model = tf.keras.models.load_model(model_path)
                experiment_data['model_path'] = model_path
                experiment_data['trained_model'] = model
                self.logger.info(f"Successfully loaded trained model: {model_path}")
            except Exception as e:
                self.logger.error(f"Failed to load model: {str(e)}")
                return None
        
        # à¹‚à¸«à¸¥à¸” model configuration
        config_path = os.path.join(self.config.LOGS_DIR, f"experiment_{experiment_id}", "model_configuration.json")
        if os.path.exists(config_path):
            try:
                import json
                with open(config_path, 'r') as f:
                    experiment_data['model_config'] = json.load(f)
                self.logger.info("Model configuration loaded successfully")
            except Exception as e:
                self.logger.warning(f"Could not load model configuration: {str(e)}")
        
        # à¹‚à¸«à¸¥à¸” preprocessing artifacts à¸–à¹‰à¸²à¸¡à¸µ
        preprocessing_path = os.path.join(self.config.RESULTS_DIR, 'preprocessing_scalers.joblib')
        if os.path.exists(preprocessing_path):
            try:
                import joblib
                experiment_data['preprocessing_artifacts'] = joblib.load(preprocessing_path)
                self.logger.info("Preprocessing artifacts loaded successfully")
            except Exception as e:
                self.logger.warning(f"Could not load preprocessing artifacts: {str(e)}")
        
        return experiment_data


def recreate_data_pipeline(config, logger):
    """
    à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸«à¸¡à¹ˆà¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™à¸•à¹ˆà¸­
    (à¹ƒà¸Šà¹‰à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¸¡à¸µ checkpoint à¹à¸•à¹ˆà¸¡à¸µà¹‚à¸¡à¹€à¸”à¸¥à¹€à¸—à¸£à¸™à¹à¸¥à¹‰à¸§)
    
    Returns:
        Dictionary à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™à¸ªà¸³à¸«à¸£à¸±à¸š step 7-8
    """
    logger.info("Recreating data pipeline for strategy testing...")
    
    # Step 1: Data Loading
    logger.info("ğŸ”„ Step 1: Reloading currency data...")
    data_loader = CurrencyDataLoader(config)
    raw_data = data_loader.load_currency_data()
    
    # Step 2: Preprocessing  
    logger.info("ğŸ”„ Step 2: Reprocessing data...")
    preprocessor = ForexDataPreprocessor(config)
    
    # à¸¥à¸­à¸‡à¹‚à¸«à¸¥à¸” preprocessing artifacts à¸à¹ˆà¸­à¸™
    try:
        preprocessor.load_preprocessing_artifacts(config.RESULTS_DIR)
        logger.info("Using existing preprocessing artifacts")
    except:
        logger.info("Creating new preprocessing artifacts")
    
    cleaned_data = preprocessor.handle_missing_values(raw_data)
    returns_data = preprocessor.calculate_percentage_returns(cleaned_data)
    normalized_data = preprocessor.apply_mixed_normalization(returns_data, fit_data=True)
    
    # Step 3: Integration
    logger.info("ğŸ”„ Step 3: Reintegrating multi-currency data...")
    integrator = MultiCurrencyIntegrator(config)
    unified_data = integrator.merge_currency_pairs(normalized_data)
    correlation_analysis = integrator.calculate_cross_correlation(unified_data)
    
    # Step 4: Sequence Preparation
    logger.info("ğŸ”„ Step 4: Recreating sequences...")
    sequence_preparator = SequenceDataPreparator(config)
    X_sequences, y_labels, timestamps = sequence_preparator.create_sequences(
        unified_data, target_pair='EURUSD'
    )
    data_splits = sequence_preparator.split_temporal_data(X_sequences, y_labels, timestamps)
    
    logger.info("âœ… Data pipeline recreation completed")
    
    return {
        'unified_data': unified_data,
        'correlation_analysis': correlation_analysis,
        'data_splits': data_splits,
        'X_sequences': X_sequences,
        'y_labels': y_labels,
        'timestamps': timestamps,
        'returns_data': returns_data  # à¹€à¸à¸´à¹ˆà¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ returns_data à¸—à¸µà¹ˆà¸¡à¸µ original prices
    }


def run_strategy_testing(config, trained_model, data_pipeline, logger):
    """
    à¸£à¸±à¸™ Step 7: Trading Strategy Testing - FIXED VERSION
    
    Args:
        config: Configuration object
        trained_model: à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸µà¹ˆà¹€à¸—à¸£à¸™à¹à¸¥à¹‰à¸§
        data_pipeline: à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ data pipeline
        logger: Logger object
        
    Returns:
        à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š strategy
    """
    logger.info("="*60)
    logger.info("ğŸ¯ STEP 7: TRADING STRATEGY TESTING")
    logger.info("="*60)
    
    strategy_manager = TradingStrategyManager(config)
    
    # à¹€à¸¥à¸·à¸­à¸à¸Šà¸¸à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¸«à¸£à¸±à¸šà¸›à¸£à¸°à¹€à¸¡à¸´à¸™
    data_splits = data_pipeline['data_splits']
    unified_data = data_pipeline['unified_data']
    returns_data = data_pipeline['returns_data']  # à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸¡à¸µ original prices
    
    if config.DEVELOPMENT_MODE:
        X_eval, y_eval, eval_timestamps = data_splits['val']
        eval_set_name = "validation"
        logger.info("ğŸ“Š Using VALIDATION set for strategy testing (Development Mode)")
    else:
        X_eval, y_eval, eval_timestamps = data_splits['test']
        eval_set_name = "test"
        logger.info("ğŸ“Š Using TEST set for final evaluation")
    
    logger.info(f"Evaluation set size: {len(X_eval)} samples")
    
    # à¸ªà¸£à¹‰à¸²à¸‡à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢à¸ˆà¸²à¸à¹‚à¸¡à¹€à¸”à¸¥
    logger.info("ğŸ¤– Generating model predictions...")
    model_predictions = trained_model.predict(X_eval, batch_size=config.BATCH_SIZE, verbose=1)
    model_predictions = model_predictions.flatten()
    
    # à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸²à¸„à¸²à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š - à¹à¸à¹‰à¹„à¸‚à¸à¸²à¸£à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡ column
    logger.info("ğŸ“‹ Extracting price data for evaluation...")
    
    # à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 1: à¸¥à¸­à¸‡à¹ƒà¸Šà¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ returns_data à¸—à¸µà¹ˆà¸¡à¸µ original prices
    try:
        # à¸Šà¸·à¹ˆà¸­ column à¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¸•à¸²à¸¡ preprocessing.py à¸„à¸·à¸­ Close_Price
        if 'EURUSD' in returns_data and 'Close_Price' in returns_data['EURUSD'].columns:
            logger.info("Using EURUSD Close_Price from returns_data")
            eurusd_data = returns_data['EURUSD']
            eval_price_data = eurusd_data.loc[eval_timestamps]['Close_Price']
        else:
            # à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 2: à¸¥à¸­à¸‡à¹ƒà¸Šà¹‰à¸Šà¸·à¹ˆà¸­ column à¸­à¸·à¹ˆà¸™
            available_columns = []
            if 'EURUSD' in returns_data:
                available_columns = list(returns_data['EURUSD'].columns)
                logger.info(f"Available EURUSD columns in returns_data: {available_columns}")
                
                # à¸«à¸²à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸—à¸µà¹ˆà¸¡à¸µ 'Close' à¹à¸¥à¸° 'Price'
                price_columns = [col for col in available_columns if 'Close' in col and 'Price' in col]
                if price_columns:
                    logger.info(f"Found price columns: {price_columns}")
                    eval_price_data = returns_data['EURUSD'].loc[eval_timestamps][price_columns[0]]
                else:
                    raise KeyError("No suitable price column found")
            else:
                raise KeyError("EURUSD data not found in returns_data")
                
    except Exception as e:
        logger.warning(f"Could not extract price data from returns_data: {str(e)}")
        
        # à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 3: à¸¥à¸­à¸‡à¹ƒà¸Šà¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ unified_data
        try:
            logger.info("Attempting to use unified_data...")
            available_unified_columns = list(unified_data.columns)
            logger.info(f"Available columns in unified_data: {available_unified_columns[:10]}...")  # à¹à¸ªà¸”à¸‡à¹à¸„à¹ˆ 10 à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹à¸£à¸
            
            # à¸«à¸²à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸à¸±à¸š EURUSD Close
            eurusd_columns = [col for col in available_unified_columns if 'EURUSD' in col and 'Close' in col]
            logger.info(f"EURUSD Close columns in unified_data: {eurusd_columns}")
            
            if eurusd_columns:
                # à¹ƒà¸Šà¹‰à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹à¸£à¸à¸—à¸µà¹ˆà¹€à¸ˆà¸­
                price_column = eurusd_columns[0]
                logger.info(f"Using column: {price_column}")
                eval_price_data = unified_data.loc[eval_timestamps][price_column]
            else:
                raise KeyError("No EURUSD Close column found in unified_data")
                
        except Exception as e2:
            logger.error(f"Could not extract price data from unified_data: {str(e2)}")
            
            # à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 4: à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸²à¸„à¸²à¸ˆà¸³à¸¥à¸­à¸‡ (fallback)
            logger.warning("Creating synthetic price data as fallback...")
            np.random.seed(42)
            base_price = 1.1000
            price_changes = np.random.normal(0, 0.001, len(eval_timestamps))
            eval_price_data = pd.Series(
                base_price + np.cumsum(price_changes), 
                index=eval_timestamps,
                name='EURUSD_Close_Synthetic'
            )
            logger.info("Using synthetic price data for strategy testing")
    
    logger.info(f"Price data extracted successfully: {len(eval_price_data)} samples")
    logger.info(f"Price range: {eval_price_data.min():.6f} - {eval_price_data.max():.6f}")
    
    logger.info("ğŸ“ˆ Testing threshold-based trading strategies...")
    
    # à¸—à¸”à¸ªà¸­à¸š threshold strategies à¸•à¹ˆà¸²à¸‡à¹†
    threshold_results = {}
    for threshold_type in ['conservative', 'moderate', 'aggressive']:
        logger.info(f"  Testing {threshold_type} strategy...")
        threshold_result = strategy_manager.apply_threshold_strategy(
            model_predictions, eval_price_data, eval_timestamps, threshold_type
        )
        threshold_results[f'multi_currency_{threshold_type}'] = threshold_result
        
        # à¹à¸ªà¸”à¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¹€à¸šà¸·à¹‰à¸­à¸‡à¸•à¹‰à¸™
        perf = threshold_result['performance']
        logger.info(f"    {threshold_type}: {perf['total_trades']} trades, "
                   f"return: {perf['total_return']:.4f}, win rate: {perf['win_rate']:.4f}")
    
    logger.info("ğŸ“Š Testing baseline strategies...")
    
    # à¸—à¸”à¸ªà¸­à¸š baseline strategies
    buy_hold_result = strategy_manager.implement_buy_hold_strategy(eval_price_data, eval_timestamps)
    rsi_result = strategy_manager.implement_rsi_strategy(eval_price_data, eval_timestamps)
    macd_result = strategy_manager.implement_macd_strategy(eval_price_data, eval_timestamps)
    
    logger.info(f"  Buy & Hold: return: {buy_hold_result['total_return']:.4f}")
    logger.info(f"  RSI: {rsi_result['performance']['total_trades']} trades, "
               f"return: {rsi_result['performance']['total_return']:.4f}")
    logger.info(f"  MACD: {macd_result['performance']['total_trades']} trades, "
               f"return: {macd_result['performance']['total_return']:.4f}")
    
    # à¸à¸²à¸£à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¹à¸šà¸šà¸„à¸£à¸šà¸–à¹‰à¸§à¸™
    logger.info("ğŸ” Performing comprehensive strategy comparison...")
    strategy_comparison = strategy_manager.compare_all_strategies(
        threshold_results['multi_currency_moderate'],
        {'buy_and_hold': buy_hold_result, 'rsi': rsi_result, 'macd': macd_result},
        {}  # Single currency comparison - placeholder
    )
    
    # à¹à¸ªà¸”à¸‡à¸œà¸¥à¸à¸²à¸£à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š
    if 'overall_rankings' in strategy_comparison:
        best_strategy = strategy_comparison['overall_rankings'].get('best_overall_strategy', 'Unknown')
        logger.info(f"ğŸ† Best overall strategy: {best_strategy}")
    
    logger.info("âœ… Strategy testing completed successfully")
    
    return {
        'model_predictions': model_predictions,
        'threshold_results': threshold_results,
        'baseline_results': {
            'buy_and_hold': buy_hold_result,
            'rsi': rsi_result,
            'macd': macd_result
        },
        'strategy_comparison': strategy_comparison,
        'eval_set_name': eval_set_name,
        'y_eval': y_eval,
        'eval_timestamps': eval_timestamps
    }


def run_visualization(config, strategy_results, data_pipeline, logger):
    """
    à¸£à¸±à¸™ Step 8: Results Analysis and Visualization
    
    Args:
        config: Configuration object
        strategy_results: à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸ˆà¸²à¸ strategy testing
        data_pipeline: à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ data pipeline
        logger: Logger object
        
    Returns:
        à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡ visualization
    """
    logger.info("="*60)
    logger.info("ğŸ“Š STEP 8: RESULTS ANALYSIS AND VISUALIZATION")
    logger.info("="*60)
    
    analyzer = ResultsAnalyzer(config)
    
    # à¸ªà¸£à¹‰à¸²à¸‡ visualizations à¸•à¹ˆà¸²à¸‡à¹†
    plots_created = []
    
    logger.info("ğŸ“ˆ Creating strategy performance visualization...")
    performance_plot = analyzer.plot_strategy_performance(
        strategy_results['strategy_comparison'], 
        f'strategy_performance_comparison_{strategy_results["eval_set_name"]}'
    )
    if performance_plot:
        plots_created.append(performance_plot)
        logger.info(f"  âœ… Performance plot: {os.path.basename(performance_plot)}")
    
    logger.info("ğŸ¯ Creating confusion matrices...")
    model_predictions_dict = {
        'CNN-LSTM': (strategy_results['model_predictions'] > 0.5).astype(int)
    }
    confusion_plot = analyzer.plot_confusion_matrices(
        model_predictions_dict, 
        strategy_results['y_eval'], 
        f'confusion_matrices_{strategy_results["eval_set_name"]}'
    )
    if confusion_plot:
        plots_created.append(confusion_plot)
        logger.info(f"  âœ… Confusion matrices: {os.path.basename(confusion_plot)}")
    
    logger.info("ğŸ”— Creating correlation analysis...")
    correlation_plot = analyzer.visualize_correlation_matrix(
        data_pipeline['correlation_analysis'], 'cross_currency_correlations'
    )
    if correlation_plot:
        plots_created.append(correlation_plot)
        logger.info(f"  âœ… Correlation plot: {os.path.basename(correlation_plot)}")
    
    logger.info("ğŸ“ Generating statistical tests...")
    statistical_results = analyzer.generate_statistical_tests(strategy_results['strategy_comparison'])
    
    # à¸ªà¸£à¹‰à¸²à¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸£à¸§à¸¡
    comprehensive_results = {
        'strategy_comparison': strategy_results['strategy_comparison'],
        'correlation_analysis': data_pipeline['correlation_analysis'],
        'statistical_tests': statistical_results,
        'evaluation_set': strategy_results['eval_set_name']
    }
    
    logger.info("ğŸŒ Creating interactive dashboard...")
    dashboard_path = analyzer.create_interactive_dashboard(
        comprehensive_results, 
        f'forex_analysis_dashboard_{strategy_results["eval_set_name"]}'
    )
    
    logger.info("ğŸ“‘ Creating research report...")
    report_path = analyzer.create_research_report(
        comprehensive_results, 
        f'{config.EXPERIMENT_NAME}_{strategy_results["eval_set_name"]}'
    )
    
    logger.info("ğŸ“„ Exporting LaTeX results...")
    latex_path = analyzer.export_results_to_latex(
        comprehensive_results, 
        f'{config.EXPERIMENT_NAME}_{strategy_results["eval_set_name"]}'
    )
    
    logger.info("âœ… Visualization and reporting completed")
    logger.info(f"  ğŸ“Š Dashboard: {os.path.basename(dashboard_path) if dashboard_path else 'Not created'}")
    logger.info(f"  ğŸ“‘ Report: {os.path.basename(report_path) if report_path else 'Not created'}")
    logger.info(f"  ğŸ“„ LaTeX: {os.path.basename(latex_path) if latex_path else 'Not created'}")
    
    return {
        'plots_created': plots_created,
        'dashboard_path': dashboard_path,
        'report_path': report_path,
        'latex_path': latex_path,
        'comprehensive_results': comprehensive_results
    }


def main():
    """
    Main execution function with enhanced resume capability
    """
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Multi-Currency CNN-LSTM Forex Prediction System')
    parser.add_argument('--experiment', type=str, help='Specific experiment ID to resume')
    parser.add_argument('--list-experiments', action='store_true', help='List available experiments')
    parser.add_argument('--from-step', type=int, choices=[7, 8], default=7, 
                       help='Start from step (7=strategy testing, 8=visualization)')
    parser.add_argument('--dev-mode', action='store_true', default=True,
                       help='Use validation set (development mode)')
    parser.add_argument('--final-mode', action='store_true',
                       help='Use test set (final evaluation mode)')
    
    args = parser.parse_args()
    
    # Initialize configuration
    config = Config()
    config.create_directories()
    
    # Handle final mode
    if args.final_mode:
        config.DEVELOPMENT_MODE = False
        print("ğŸš¨ FINAL EVALUATION MODE: Using test set")
    else:
        config.DEVELOPMENT_MODE = True
        print("ğŸ”¬ DEVELOPMENT MODE: Using validation set")
    
    # Initialize experiment manager
    exp_manager = ExperimentManager(config)
    
    # Handle list experiments request
    if args.list_experiments:
        experiments = exp_manager.find_existing_experiments()
        print(f"\nğŸ“‹ Available experiments ({len(experiments)} found):")
        
        if not experiments:
            print("  No experiments found.")
            return 0
        
        for i, exp in enumerate(experiments, 1):
            print(f"\n  {i}. {exp['experiment_id']}")
            print(f"     Created: {exp['created_time']}")
            print(f"     Model: {exp['model_size_mb']:.1f} MB")
            print(f"     Training completed: {'âœ…' if exp['has_training_completed'] else 'âŒ'}")
            
            completed_steps = [k for k, v in exp['completed_steps'].items() if v]
            print(f"     Completed steps: {', '.join(completed_steps)}")
        
        return 0
    
    # Find experiment to use
    if args.experiment:
        # Use specified experiment
        experiment_id = args.experiment
        config.update_experiment_name(experiment_id)
        print(f"ğŸ¯ Using specified experiment: {experiment_id}")
    else:
        # Use latest experiment
        latest_exp = exp_manager.get_latest_experiment()
        if not latest_exp:
            print("âŒ No trained experiments found!")
            print("   Run with --list-experiments to see available experiments")
            return 1
        
        experiment_id = latest_exp['experiment_id']
        config.update_experiment_name(experiment_id)
        print(f"ğŸ”„ Resuming latest experiment: {experiment_id}")
        print(f"   Created: {latest_exp['created_time']}")
        print(f"   Model size: {latest_exp['model_size_mb']:.1f} MB")
    
    # Setup logging
    logger = setup_logging(config.LOG_LEVEL, config.get_log_path('resume_execution'))
    
    print_system_info()
    
    print("=" * 80)
    print("ğŸš€ MULTI-CURRENCY CNN-LSTM FOREX PREDICTION SYSTEM")
    print("=" * 80)
    print(f"Experiment ID: {experiment_id}")
    print(f"Mode: {'Final Evaluation' if not config.DEVELOPMENT_MODE else 'Development'}")
    print(f"Starting from step: {args.from_step}")
    print(f"Starting execution at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    start_time = datetime.now()
    
    try:
        # Load experiment data
        logger.info("Loading existing experiment data...")
        experiment_data = exp_manager.load_experiment_data(experiment_id)
        
        if not experiment_data or not experiment_data.get('trained_model'):
            logger.error("Could not load trained model!")
            return 1
        
        trained_model = experiment_data['trained_model']
        logger.info(f"âœ… Successfully loaded trained model")
        
        # Display model information
        if experiment_data.get('model_config'):
            model_config = experiment_data['model_config']
            print(f"ğŸ“Š Model Information:")
            print(f"   Total parameters: {model_config.get('total_parameters', 'Unknown'):,}")
            print(f"   Architecture: {model_config.get('model_name', 'Unknown')}")
            print(f"   Input shape: {model_config.get('input_shape', 'Unknown')}")
        
        # Recreate data pipeline
        logger.info("Recreating data pipeline...")
        data_pipeline = recreate_data_pipeline(config, logger)
        
        results = {}
        
        # Run requested steps
        if args.from_step <= 7:
            logger.info("Starting Step 7: Strategy Testing...")
            strategy_results = run_strategy_testing(config, trained_model, data_pipeline, logger)
            results['strategy_results'] = strategy_results
            
            # Show key results
            print(f"\nğŸ¯ STRATEGY TESTING RESULTS:")
            moderate_perf = strategy_results['threshold_results']['multi_currency_moderate']['performance']
            print(f"   Multi-Currency CNN-LSTM (Moderate):")
            print(f"     Total Return: {moderate_perf['total_return']:.4f}")
            print(f"     Win Rate: {moderate_perf['win_rate']:.4f}")
            print(f"     Sharpe Ratio: {moderate_perf['sharpe_ratio']:.4f}")
            print(f"     Total Trades: {moderate_perf['total_trades']}")
            
            if 'overall_rankings' in strategy_results['strategy_comparison']:
                best_strategy = strategy_results['strategy_comparison']['overall_rankings'].get('best_overall_strategy', 'Unknown')
                print(f"   ğŸ† Best Overall Strategy: {best_strategy}")
        
        if args.from_step <= 8:
            if 'strategy_results' not in results:
                logger.error("Strategy testing results not available for visualization")
                return 1
            
            logger.info("Starting Step 8: Visualization...")
            viz_results = run_visualization(config, results['strategy_results'], data_pipeline, logger)
            results['visualization_results'] = viz_results
            
            # Show generated files
            print(f"\nğŸ“Š GENERATED RESULTS:")
            if viz_results['dashboard_path']:
                print(f"   ğŸ“ˆ Interactive Dashboard: {os.path.basename(viz_results['dashboard_path'])}")
            if viz_results['report_path']:
                print(f"   ğŸ“‘ Research Report: {os.path.basename(viz_results['report_path'])}")
            if viz_results['latex_path']:
                print(f"   ğŸ“„ LaTeX Results: {os.path.basename(viz_results['latex_path'])}")
        
        # Final summary
        end_time = datetime.now()
        duration = end_time - start_time
        
        print("\n" + "=" * 80)
        print("ğŸ‰ EXECUTION COMPLETED SUCCESSFULLY!")
        print("=" * 80)
        print(f"Total execution time: {str(duration).split('.')[0]}")
        print(f"Results saved to: {config.RESULTS_DIR}")
        
        if config.DEVELOPMENT_MODE:
            print("\nğŸ’¡ DEVELOPMENT MODE NOTICE:")
            print("   This analysis used the VALIDATION set.")
            print("   For final thesis results, run with --final-mode")
        else:
            print("\nğŸ“ FINAL EVALUATION COMPLETED")
            print("   Results ready for thesis defense!")
        
        print("=" * 80)
        
        return 0
        
    except Exception as e:
        logger.error(f"Execution failed: {str(e)}")
        logger.error("Full traceback:")
        logger.error(traceback.format_exc())
        
        print(f"\nâŒ Execution failed: {str(e)}")
        print("Check the log files for detailed error information.")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)